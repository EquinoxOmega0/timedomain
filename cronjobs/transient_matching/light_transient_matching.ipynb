{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --user alerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# light_transient_matching\n",
    "## Matches DESI observations to ALERCE and DECAM ledger objects\n",
    "\n",
    "This code predominately takes in data from the ALERCE and DECAM ledger brokers and identifies DESI observations within 2 arcseconds of those objects, suspected to be transients. It then prepares those matches to be fed into our [CNN code](https://github.com/MatthewPortman/timedomain/blob/master/cronjobs/transient_matching/modified_cnn_classify_data_gradCAM.ipynb) which attempts to identify the class of these transients.\n",
    "\n",
    "The main matching algorithm uses astropy's **match_coordinate_sky** to match 1-to-1 targets with the objects from the two ledgers. Wrapping functions handle data retrieval from both the ledgers as well as from DESI and prepare this data to be fed into **match_coordinate_sky**. Since ALERCE returns a small enough (pandas) dataframe, we do not need to precondition the input much. However, DECAM has many more objects to match so we use a two-stage process: an initial 2 degree match to tile RA's/DEC's and a second closer 1 arcsecond match to individual targets. \n",
    "\n",
    "As the code is a work in progress, please forgive any redundancies. We are attempting to merge all of the above (neatly) into the same two or three matching/handling functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky, Angle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Some handy global variables\n",
    "global db_filename\n",
    "db_filename = '/global/cfs/cdirs/desi/science/td/daily-search/transients_search.db'\n",
    "global exposure_path\n",
    "exposure_path = os.environ[\"DESI_SPECTRO_REDUX\"]\n",
    "global color_band\n",
    "color_band = \"r\"\n",
    "\n",
    "global today\n",
    "today = Time.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the file names\n",
    "def all_candidate_filenames(transient_dir: str):\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob.glob(transient_dir + \"/*.fits\") # Hardcoding is hopefully a temporary measure.\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the transient spectra directory:\")\n",
    "        print(transient_dir)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ALeRCE_ledgermaker https://github.com/alercebroker/alerce_client\n",
    "# I have had trouble importing this before so I copy, paste it, and modify it here.\n",
    "\n",
    "# I also leave these imports here because why not?\n",
    "import requests\n",
    "from alerce.core import Alerce\n",
    "from alerce.exceptions import APIError\n",
    "\n",
    "alerce_client = Alerce()\n",
    "\n",
    "def access_alerts(lastmjd_in=None, classifier='stamp_classifier', class_names=['SN', 'AGN']):\n",
    "    if type(class_names) is not list:\n",
    "        raise TypeError('Argument `class_names` must be a list.')\n",
    "        \n",
    "    dataframes = []\n",
    "    if not lastmjd_in:\n",
    "        date_range = 60\n",
    "        lastmjd_in = Time.now().mjd - 60\n",
    "        print('Defaulting to a lastmjd range of', str(date_range), 'days before today.')\n",
    "        \n",
    "    for class_name in class_names:\n",
    "        data = alerce_client.query_objects(classifier=classifier,\n",
    "                                           class_name=class_name, \n",
    "                                           order_by='oid',\n",
    "                                           order_mode='DESC',\n",
    "                                           page_size=5000,\n",
    "                                           lastmjd=lastmjd_in,\n",
    "                                           format='pandas')\n",
    "        \n",
    "        #if lastmjd is not None:\n",
    "        #    select = data['lastmjd'] >= lastmjd\n",
    "        #    data = data[select]\n",
    "            \n",
    "        dataframes.append(data)\n",
    "    \n",
    "    return pd.concat(dataframes).sort_values(by='lastmjd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_TAMU_ledgermaker.ipynb\n",
    "# Function to grab decam data\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "def access_decam_data(url, overwrite=False):\n",
    "    \"\"\"Download reduced DECam transient data from Texas A&M.\n",
    "    Cache the data to avoid lengthy and expensive downloads.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL for accessing the data.\n",
    "    overwrite : bool\n",
    "        Download new data and overwrite the cached data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    decam_transients : pandas.DataFrame\n",
    "        Table of transient data.\n",
    "    \"\"\"\n",
    "    folders = url.split('/')\n",
    "    thedate = folders[-1] if len(folders[-1]) > 0 else folders[-2]\n",
    "    outfile = '{}.csv'.format(thedate)\n",
    "    \n",
    "    if os.path.exists(outfile) and not overwrite:\n",
    "        # Access cached data.\n",
    "        decam_transients = pd.read_csv(outfile)\n",
    "    else:\n",
    "        # Download the DECam data index.\n",
    "        # A try/except is needed because the datahub SSL certificate isn't playing well with URL requests.\n",
    "        try:\n",
    "            decam_dets = requests.get(url, auth=('decam','tamudecam')).text\n",
    "        except:\n",
    "            requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "            decam_dets = requests.get(url, verify=False, auth=('decam','tamudecam')).text\n",
    "            \n",
    "        # Convert transient index page into scrapable data using BeautifulSoup.\n",
    "        soup = BeautifulSoup(decam_dets)\n",
    "        \n",
    "        # Loop through transient object summary JSON files indexed in the main transient page.\n",
    "        # Download the JSONs and dump the info into a Pandas table.\n",
    "        decam_transients = None\n",
    "        j = 0\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'object-summary.json' in a:\n",
    "                link = a['href'].replace('./', '')\n",
    "                summary_url  = url + link        \n",
    "                summary_text = requests.get(summary_url, verify=False, auth=('decam','tamudecam')).text\n",
    "                summary_data = json.loads(summary_text)\n",
    "\n",
    "                j += 1\n",
    "                #print('Accessing {:3d}  {}'.format(j, summary_url)) # Modified by Matt\n",
    "\n",
    "                if decam_transients is None:\n",
    "                    decam_transients = pd.DataFrame(summary_data, index=[0])\n",
    "                else:\n",
    "                    decam_transients = pd.concat([decam_transients, pd.DataFrame(summary_data, index=[0])])\n",
    "                    \n",
    "        # Cache the data for future access.\n",
    "        print('Saving output to {}'.format(outfile))\n",
    "        decam_transients.to_csv(outfile, index=False)\n",
    "        \n",
    "    return decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in fits table info, RA, DEC, MJD and targetid if so desired\n",
    "# Uses control parameter tile to determine if opening tile exposure file or not since headers are different\n",
    "def read_fits_info(filepath: str, tile = True):\n",
    "    \n",
    "    if tile:\n",
    "        hdu_num = 5\n",
    "    else:\n",
    "        hdu_num = 1\n",
    "    \n",
    "    try:\n",
    "        with fits.open(filepath) as hdu1:\n",
    "    \n",
    "            data_table = Table(hdu1[hdu_num].data) #columns\n",
    "        \n",
    "            targ_id = data_table['TARGETID']\n",
    "            targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "            targ_dec = data_table['TARGET_DEC'].data\n",
    "            #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "            \n",
    "            if tile:\n",
    "                targ_mjd = hdu1[hdu_num].header['MJD-OBS']\n",
    "            \n",
    "    except:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        print(\"Could not open or use:\", filename)\n",
    "        print(\"In path:\", filepath)\n",
    "        print(\"Trying the next file...\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    if not tile:\n",
    "        targ_mjd = filepath.split(\"/\")[-1].split(\"_\")[-2] #to grab the date\n",
    "        targ_mjd = targ_mjd[:4]+\"-\"+targ_mjd[4:6]+\"-\"+targ_mjd[6:] # Adding dashes for Time\n",
    "        targ_mjd = Time(targ_mjd).mjd\n",
    "    \n",
    "    return targ_ra, targ_dec, targ_mjd, targ_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching function\n",
    "\n",
    "Mor or less the prototype to the later rendition used for DECAM. Will not be around in later versions of this notebook as I will be able to repurpose the DECAM code to do both. Planned obsolescence? \n",
    "\n",
    "It may not be even worth it at this point... ah well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype for the later, heftier matching function\n",
    "# Will be deprecated, please reference commentary in inner_matching later for operation notes\n",
    "def matching(path_in: str, max_sep: float, tile = False, date_dict = {}): \n",
    "    \n",
    "    max_sep *= u.arcsec\n",
    "    \n",
    "    #if not target_ra_dec_date:\n",
    "    #    target_ras, target_decs, obs_mjds = read_fits_ra_dec(path_in, tile)\n",
    "    #else:\n",
    "    #    target_ras, target_decs, obs_mjds = target_ra_dec_date\n",
    "        \n",
    "    if not date_dict:\n",
    "        print(\"No RA's/DEC's fed in. Quitting.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    for obs_mjd, ra_dec in date_dict.items():\n",
    "        try:\n",
    "            alerts = access_alerts(lastmjd_in = obs_mjd - 28, class_names = ['SN']) # Modified Julian Day .mjd \n",
    "        except:\n",
    "            print(\"No SN matches (28 day range) for\", obs_mjd)\n",
    "            continue\n",
    "    \n",
    "        # For each fits file, look at one month before the observation from Alerce\n",
    "        tree_name = \"kdtree_\" + str(obs_mjd - 28)\n",
    "\n",
    "        alerts_ra = alerts['meanra'].to_numpy()\n",
    "        alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "        # Grab RAs and DECs from input. \n",
    "        # I just haven't used numpy in so long...\n",
    "        target_ras = ra_dec[:, 0]#np.array([i[0] for i in ra_dec], dtype = np.float64)\n",
    "        target_decs = ra_dec[:, 1]\n",
    "        target_ids = np.int64(ra_dec[:, 2])\n",
    "\n",
    "        # Check for NaN's and remove which don't play nice with match_coordinates_sky\n",
    "        nan_ra = np.isnan(target_ras)\n",
    "        nan_dec = np.isnan(target_decs)\n",
    "\n",
    "        if np.any(nan_ra) or np.any(nan_dec):\n",
    "            print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "            #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "            nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "            target_ras = target_ras[nans] # Logic masking, probably more efficient\n",
    "            target_decs = target_decs[nans]\n",
    "            #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "\n",
    "        # Converting to SkyCoord type arrays (really quite handy)\n",
    "        coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "        coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "        idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "        # Filtering by maximum separation and closest match\n",
    "        sep_constraint = d2d_trans < max_sep\n",
    "        trans_matches = coo_trans_search[sep_constraint]\n",
    "        alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "        \n",
    "        targetid_matches = target_ids[sep_constraint]\n",
    "\n",
    "        # Adding everything to lists and outputting\n",
    "        if trans_matches.size:\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alerts_matches)\n",
    "            sort_dist = np.sort(d2d_trans)\n",
    "            print(\"Minimum distance found: \", sort_dist[0])\n",
    "            print()        \n",
    "\n",
    "    return all_trans_matches, all_alerts_matches, targetid_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to ALERCE \n",
    "Runs a 5 arcsecond match of DESI to Alerce objects. Since everything is handled in functions, this part is quite clean.\n",
    "\n",
    "From back when I was going to use *if __name__ == \"__main__\":*... those were the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Transient dir\n",
    "    path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "    # Grab paths\n",
    "    paths_to_fits = all_candidate_filenames(path_to_transient)\n",
    "    \n",
    "    desi_info_dict = {}\n",
    "    \n",
    "    '''\n",
    "    To be used when functions are properly combined.\n",
    "    initial_check(ledger_df = None, ledger_type = '')\n",
    "    closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = [])\n",
    "    '''\n",
    "    \n",
    "    # Iterate through every fits file and grab all necessary info and plop it all together\n",
    "    for path in paths_to_fits:\n",
    "        target_ras, target_decs, obs_mjd, targ_ids = read_fits_info(path, tile = False)\n",
    "        \n",
    "        try: \n",
    "            np.append(desi_info_dict[obs_mjd], [target_ras], [target_decs], [targ_ids], axis = 0)\n",
    "            #desi_info_dict[obs_mjd].extend((target_ras, target_decs, targ_ids))\n",
    "        except:\n",
    "            desi_info_dict[obs_mjd] = np.column_stack((target_ras, target_decs, targ_ids))\n",
    "            #desi_info_dict[obs_mjd].append((target_ras, target_decs, targ_ids))\n",
    "    #trans_matches, _ = matching(path, 5.0, (all_desi_ras, all_desi_decs, all_obs_mjd))\n",
    "\n",
    "#     if trans_matches.size:\n",
    "#         all_trans_matches.append(trans_matches)\n",
    "#         all_alerts_matches.append(alerts_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SN matches (28 day range) for 59378.0\n",
      "No SN matches (28 day range) for 59377.0\n",
      "No SN matches (28 day range) for 59379.0\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# I was going to prepare everything by removing duplicate target ids but it's more trouble than it's worth and match_coordinates_sky can handle it\n",
    "# Takes quite a bit of time... not much more I can do to speed things up though since querying Alerce for every individual date is the hang-up.\n",
    "trans_matches, _, target_id_matches = matching(\"\", 5.0, date_dict = desi_info_dict)\n",
    "print(trans_matches)\n",
    "print(target_id_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to DECAM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the frame fits files\n",
    "def glob_frames(exp_d: str):    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob.glob(exp_d + \"/cframe-\" + color_band + \"*.fits\") # Only need one of b, r, z\n",
    "        # sframes not flux calibrated\n",
    "        # May want to use tiles... coadd (will need later, but not now)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the exposure directory:\")\n",
    "        print(exp_d)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes match information to file\n",
    "def write_matches_to_file(start_date, end_date, all_matches_dict, ledger_type):\n",
    "    filename = \"./matches_\" + ledger_type + \"_\" + start_date + \"-\" + end_date + \".txt\"\n",
    "    \n",
    "    with open(filename, 'w') as mfile:\n",
    "        \n",
    "        # Formatting\n",
    "        mfile.write(\"date:\\n\")\n",
    "        # Column names\n",
    "        # Still have to add fiber id, distance between TAMU and fiber obj\n",
    "        mfile.write(\"\\tframename; (RA, DEC); Table Index in FIBERMAP; Ledger ID; Alert (RA, DEC) - matched to 2\\\"\\n\") \n",
    "        \n",
    "        # Iterating through matches dictionary\n",
    "        for date, row in all_matches_dict.items():\n",
    "            if row: # Just in case\n",
    "                mfile.write(str(date) + \": \\n\")\n",
    "                \n",
    "                # Can be multiple matches per date hence the loop\n",
    "                for i in row:\n",
    "                    mfile.write(\"\\t\")\n",
    "                    \n",
    "                    # See above for what each part means\n",
    "                    mfile.write(\"; \".join(str(x) for x in i))\n",
    "                    mfile.write(\"\\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match handling routines\n",
    "\n",
    "The two functions below perform data handling/calling for the final match step. \n",
    "\n",
    "The first, **initial_check** grabs all the tile RAs and DECS from the exposures and tiles SQL table, does some filtering, and sends the necessary information to the matching function. Currently designed to handle ALERCE as well but work has to be done to make sure it operates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_check(ledger_df = None, ledger_type = ''):\n",
    "\n",
    "    query_date_start = \"20201130\"\n",
    "    \n",
    "    #today = Time.now()\n",
    "    smushed_YMD = today.iso.split(\" \")[0].replace(\"-\",\"\")\n",
    "    \n",
    "    query_date_end = smushed_YMD \n",
    "\n",
    "    # Handy queries for debugging/useful info\n",
    "    query2 = \"PRAGMA table_info(exposures)\"\n",
    "    query3 = \"PRAGMA table_info(tiles)\"\n",
    "    # Crossmatch across tiles and exposures to grab obsdate via tileid\n",
    "    query_match = \"SELECT distinct tilera, tiledec, obsdate, obsmjd, expid, exposures.tileid from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        query_date_start + \" AND \" + query_date_end + \";\" \n",
    "    \n",
    "    '''\n",
    "    Some handy code for debugging\n",
    "    #cur.execute(query2)\n",
    "    #row2 = cur.fetchall()\n",
    "    #for i in row2:\n",
    "    #    print(i[:])\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Querying sql and returning a data type called sqlite3 row, it's kind of like a namedtuple/dictionary\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "\n",
    "    conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(query_match)\n",
    "    matches_list = cur.fetchall()\n",
    "    cur.close()\n",
    "\n",
    "    # I knew there was a way! THANK YOU!\n",
    "    # https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension\n",
    "    \n",
    "    # Grabbing everything by obsdate from matches_list\n",
    "    date_dict = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "\n",
    "    alert_matches_dict = {}\n",
    "\n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    # Grabbing DECAM ledger if not already fed in\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "\n",
    "    # Iterating through the dates and checking each tile observed on each date\n",
    "    # It is done in this way to cut down on calls to ALERCE since we go day by day\n",
    "    # It's also a convenient way to organize things\n",
    "    for date, row in date_dict.items():\n",
    "        \n",
    "        date_str = str(date)\n",
    "        date_str = date_str[:4]+\"-\"+date_str[4:6]+\"-\"+date_str[6:] # Adding dashes for Time\n",
    "        obs_mjd = Time(date_str).mjd\n",
    "\n",
    "        # This method is *technically* safer than doing a double list comprehension with set albeit slower\n",
    "        # The lists are small enough that speed shouldn't matter here\n",
    "        unique_tileid = {i[-1]:(i[0], i[1]) for i in row}\n",
    "        exposure_ras, exposure_decs = zip(*unique_tileid.values())\n",
    "        \n",
    "        # Grabbing alerce ledger if not done already\n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd = obs_mjd - 28) # Modified Julian Day #.mjd\n",
    "        elif ledger_type.upper() == 'DECAM_TAMU':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Cannot use alerts broker/ledger provided. Stopping before match.\")\n",
    "            return {}\n",
    "        \n",
    "        # Where the magic/matching happens\n",
    "        trans_matches, alert_matches = inner_matching(target_ras, target_decs, obs_mjd, '', max_sep = 1.8, sep_units = 'deg', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "        \n",
    "        # Add everything into one giant list for both\n",
    "        if trans_matches.size:\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alert_matches)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Prepping output\n",
    "        # Populating the dictionary by date (a common theme)\n",
    "        # Each element in the dictionary thus contains the entire sqlite3 row (all info from sql tables with said headers)\n",
    "        alert_matches_dict[date] = []\n",
    "\n",
    "        for tup in trans_matches:\n",
    "            ra = tup.ra.deg\n",
    "            dec = tup.dec.deg\n",
    "            match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra, dec)] # Just rebuilding for populating, this shouldn't change/exclude anything\n",
    "            alert_matches_dict[date].extend(match_rows)\n",
    "            \n",
    "    return alert_matches_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## closer_check\n",
    "**closer_check** is also a handling function but operates differently in that now it is checking individual targets. This *must* be run after **initial_check** because it takes as input the dictionary **initial_check** spits out. It then grabs all the targets from the DESI files and pipes that into the matching function but this time with a much more strict matching radius (in this case 2 arcseconds). \n",
    "\n",
    "It then preps the data for output and writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = []):\n",
    "    all_exp_matches = {}\n",
    "        \n",
    "    if not matches_dict:\n",
    "        print(\"No far matches fed in for nearby matching. Returning none.\")\n",
    "        return {}\n",
    "    \n",
    "    # Again just in case the dataframe isn't fed in\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        \n",
    "        id_head = 'ObjectID'\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "        \n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    \n",
    "    # Iterating through date and all tile information for that date\n",
    "    for date, row in matches_dict.items(): \n",
    "        print(\"\\n\", date)\n",
    "        if date in exclusion_list:\n",
    "            continue\n",
    "\n",
    "        # Declaring some things\n",
    "        all_exp_matches[date] = []\n",
    "        alert_exp_matches = []\n",
    "        file_indices = {}\n",
    "\n",
    "        all_targ_ras = np.array([])\n",
    "        all_targ_decs = np.array([])\n",
    "\n",
    "        # Iterating through each initial match tile for every date\n",
    "        for i in row:\n",
    "            # Grabbing the paths and iterating through them to grab the RA's/DEC's\n",
    "            exp_paths = '/'.join((exposure_path, \"daily/exposures\", str(i['obsdate']), \"000\"+str(i['expid'])))\n",
    "            #print(exp_paths)\n",
    "            for path in glob_frames(exp_paths):\n",
    "                #print(path)\n",
    "                targ_ras, targ_decs, _ = read_fits_ra_dec(path, tile = True)\n",
    "\n",
    "                # This is to retain the row to debug/check the original FITS file\n",
    "                # And to pull the info by row direct if you feel so inclined\n",
    "                all_len = len(all_targ_ras)\n",
    "                new_len = len(targ_ras)\n",
    "                if all_len:\n",
    "                    all_len -= 1\n",
    "                    file_indices[path] = (all_len, all_len + new_len) # The start and end index, modulo number\n",
    "                else:\n",
    "                    file_indices[path] = (0, new_len) # The start and end index, modulo number\n",
    "\n",
    "                if len(targ_ras) != len(targ_decs):\n",
    "                    print(\"Length of all ras vs. all decs do not match.\")\n",
    "                    print(\"Something went wrong!\")\n",
    "                    print(\"Continuing but not adding those to match...\")\n",
    "                    continue\n",
    "\n",
    "                # All the ras/decs together!\n",
    "                all_targ_ras = np.append(all_targ_ras, targ_ras)\n",
    "                all_targ_decs = np.append(all_targ_decs, targ_decs)\n",
    "\n",
    "        date_mjd = str(date)[:4]+\"-\"+str(date)[4:6] + \"-\" + str(date)[6:] # Adding dashes for Time\n",
    "        date_mjd = Time(date_mjd).mjd\n",
    "        \n",
    "        # Grabbing ALERCE just in case\n",
    "        # Slow\n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            \n",
    "            id_head = 'oid'\n",
    "            ra_head = 'meanra'\n",
    "            dec_head = 'meandec'\n",
    "            \n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd_in = obs_mjd - 28) # Modified Julian Day #.mjd\n",
    "        \n",
    "        # Where the magic matching happens. This time with separation 2 arcseconds.\n",
    "        # Will be cleaned up (eventually)\n",
    "        alert_exp_matches, alerts_matches = inner_matching(all_targ_ras, all_targ_decs, date_mjd, '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "\n",
    "        # Iterating through the succesful DESI matches\n",
    "        for match_idx in range(len(alert_exp_matches)):\n",
    "            # Grabbing RA, DEC\n",
    "            match_ra = alert_exp_matches[match_idx].ra.deg\n",
    "            match_dec = alert_exp_matches[match_idx].dec.deg\n",
    "            \n",
    "            # Using indexing to determine location in fits file\n",
    "            location = np.where(match_ra == all_targ_ras)[0][0] # VERY unlikely to have a duplicate in the RA, I think this is safe\n",
    "            \n",
    "            alert_ra = alerts_matches[match_idx].ra.deg\n",
    "            alert_dec = alerts_matches[match_idx].dec.deg\n",
    "            \n",
    "            # From meanra column, match ra, then grab the location, specify the key 'oid', grab the values from that 'series'\n",
    "            # should only have one match so we can grab the first\n",
    "            ledger_ID = ledger_df.loc[ledger_df[ra_head] == alert_ra][id_head].values[0] # I think this too is safe           \n",
    "            \n",
    "            # Iterate through file indices to find the row of the match\n",
    "            # Not super intuitive but you do what you gotta do\n",
    "            for k, v in file_indices.items():\n",
    "                if location in range(v[0], v[1]):\n",
    "                    # filepath, (ra,dec) for match, loc + 1 because fits indexing starts at 1, Ledger ID, (RA, DEC) for ledger table\n",
    "                    match_info = (k.split(\"/\")[-1], (match_ra, match_dec), (loc + 1) % (v[1] - v[0]), ledger_ID, (alert_ra, alert_dec)) \n",
    "                    # Since it's a pain to retrieve the index from the table, it'll be easier to match it after the fact with np.where\n",
    "                    \n",
    "                    # To avoid duplicates\n",
    "                    if match_info not in all_exp_matches[date]:\n",
    "                        all_exp_matches[date].append(match_info)\n",
    "    \n",
    "    # Does not easily output to a csv since we have multiple results for each date\n",
    "    # so uh... custom file output for me\n",
    "    return all_exp_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decam_matching\n",
    "#### aka the bread & butter\n",
    "**decam_matching** is what ultimately does the final match and calls **match_coordinates_sky** with everything fed in. So really it doesn't do much other than take in all the goodies and make everyone happy.\n",
    "\n",
    "It may still be difficult to co-opt for alerce matching but that may be a project for another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_matching(target_ras_in = np.array([]), target_decs_in = np.array([]), obs_mjd_in = '', path_in = '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = None, ledger_type_in = ''): # to be combined with the other matching thing in due time\n",
    "    \n",
    "    # Figuring out the units\n",
    "    if sep_units == 'arcsec':\n",
    "        max_sep *= u.arcsec\n",
    "    elif sep_units == 'arcmin':\n",
    "        max_sep *= u.arcmin\n",
    "    elif sep_units == 'deg':\n",
    "        max_sep *= u.deg\n",
    "    else:\n",
    "        print(\"Separation unit specified is invalid for matching. Defaulting to arcsecond.\")\n",
    "        max_sep *= u.arcsec\n",
    "        \n",
    "    if not np.array(target_ras_in).size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Checking for NaNs, again doesn't play nice with match_coordinates_sky\n",
    "    nan_ra = np.isnan(target_ras_in)\n",
    "    nan_dec = np.isnan(target_decs_in)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras_in = target_ras_in[nans] # Logic masking, probably more efficient\n",
    "        target_decs_in = target_decs_in[nans]\n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "\n",
    "    # For quick matching if said kdtree actually does anything\n",
    "    # Supposed to speed things up on subsequent runs *shrugs*\n",
    "    tree_name = \"_\".join((\"kdtree\", ledger_type_in, str(obs_mjd_in - 28)))\n",
    "    \n",
    "    # Selecting header string to use with the different alert brokers/ledgers\n",
    "    if ledger_type_in.upper() == 'DECAM_TAMU':\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "    \n",
    "    elif ledger_type_in.upper() == 'ALERCE':\n",
    "        ra_head = 'meanra'\n",
    "        dec_head = 'meandec'\n",
    "        \n",
    "    else:\n",
    "        print(\"No ledger type specified. Quitting.\") \n",
    "        # lofty goals\n",
    "        # Will try to figure it out assuming it's a pandas dataframe.\")\n",
    "        #print(\"Returning empty-handed for now until that is complete - Matthew P.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Convert df RA/DEC to numpy arrays\n",
    "    alerts_ra = ledger_df_in[ra_head].to_numpy()\n",
    "    alerts_dec = ledger_df_in[dec_head].to_numpy()\n",
    "\n",
    "    # Convert everything to SkyCoord\n",
    "    coo_trans_search = SkyCoord(target_ras_in*u.deg, target_decs_in*u.deg)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    # Do the matching! \n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "    # Filter out the good stuff\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        print(len(trans_matches), \"matches with separation -\", max_sep)\n",
    "        #sort_dist = np.sort(d2d_trans)\n",
    "        #print(\"Minimum distance found: \", sort_dist[0])\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab DECAM ledger as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to LCData_Legacy.csv\n"
     ]
    }
   ],
   "source": [
    "decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/', overwrite = True) # If True, grabs a fresh batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectID</th>\n",
       "      <th>RA-OBJECT</th>\n",
       "      <th>DEC-OBJECT</th>\n",
       "      <th>NumberAlerts</th>\n",
       "      <th>MaxSCORE</th>\n",
       "      <th>RA-PSEUDO-HOST</th>\n",
       "      <th>DEC-PSEUDO-HOST</th>\n",
       "      <th>SEP-PSEUDO-HOST</th>\n",
       "      <th>RA-NEIGHBOR-STAR</th>\n",
       "      <th>DEC-NEIGHBOR-STAR</th>\n",
       "      <th>...</th>\n",
       "      <th>Discovery-Round</th>\n",
       "      <th>Discovery-Time</th>\n",
       "      <th>Discovery-Filter</th>\n",
       "      <th>Discovery-Magnitude</th>\n",
       "      <th>Discovery-SNR</th>\n",
       "      <th>Latest-Round</th>\n",
       "      <th>Latest-Time</th>\n",
       "      <th>Latest-Filter</th>\n",
       "      <th>Latest-Magnitude</th>\n",
       "      <th>Latest-SNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221407558m001825</td>\n",
       "      <td>211.982786</td>\n",
       "      <td>-0.306951</td>\n",
       "      <td>12</td>\n",
       "      <td>0.972</td>\n",
       "      <td>211.982614</td>\n",
       "      <td>-0.306946</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>211.983372</td>\n",
       "      <td>-0.306315</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:40:19.074</td>\n",
       "      <td>N</td>\n",
       "      <td>22.13</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-18T05:37:55.763</td>\n",
       "      <td>N</td>\n",
       "      <td>22.86</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408139m033502</td>\n",
       "      <td>212.057952</td>\n",
       "      <td>-3.583947</td>\n",
       "      <td>28</td>\n",
       "      <td>0.954</td>\n",
       "      <td>212.057864</td>\n",
       "      <td>-3.583960</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>212.058798</td>\n",
       "      <td>-3.586276</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:20:58.209</td>\n",
       "      <td>N</td>\n",
       "      <td>21.78</td>\n",
       "      <td>25.7</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T06:00:51.873</td>\n",
       "      <td>N</td>\n",
       "      <td>21.82</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408412p002445</td>\n",
       "      <td>212.171737</td>\n",
       "      <td>0.412527</td>\n",
       "      <td>49</td>\n",
       "      <td>0.998</td>\n",
       "      <td>212.171673</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>212.174697</td>\n",
       "      <td>0.411566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:36:50.928</td>\n",
       "      <td>S</td>\n",
       "      <td>20.61</td>\n",
       "      <td>36.2</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:59:21.867</td>\n",
       "      <td>S</td>\n",
       "      <td>21.46</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408578m005300</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883400</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>212.239800</td>\n",
       "      <td>-0.884900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:17:30.880</td>\n",
       "      <td>S</td>\n",
       "      <td>22.55</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-24T07:10:51.368</td>\n",
       "      <td>S</td>\n",
       "      <td>22.33</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221409059m023156</td>\n",
       "      <td>212.274757</td>\n",
       "      <td>-2.532478</td>\n",
       "      <td>21</td>\n",
       "      <td>0.969</td>\n",
       "      <td>212.274533</td>\n",
       "      <td>-2.532531</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>212.275356</td>\n",
       "      <td>-2.535003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:14:02.747</td>\n",
       "      <td>N</td>\n",
       "      <td>22.27</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>2021-05-18T03:17:17.544</td>\n",
       "      <td>N</td>\n",
       "      <td>22.99</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021437535m004239</td>\n",
       "      <td>219.473273</td>\n",
       "      <td>-0.710944</td>\n",
       "      <td>6</td>\n",
       "      <td>0.973</td>\n",
       "      <td>219.473277</td>\n",
       "      <td>-0.711253</td>\n",
       "      <td>1.1100</td>\n",
       "      <td>219.473195</td>\n",
       "      <td>-0.714488</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:37:33.855</td>\n",
       "      <td>N</td>\n",
       "      <td>21.46</td>\n",
       "      <td>30.6</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:15:30.763</td>\n",
       "      <td>N</td>\n",
       "      <td>21.44</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021440559m025615</td>\n",
       "      <td>220.233290</td>\n",
       "      <td>-2.937658</td>\n",
       "      <td>4</td>\n",
       "      <td>0.940</td>\n",
       "      <td>220.233839</td>\n",
       "      <td>-2.935933</td>\n",
       "      <td>6.5148</td>\n",
       "      <td>220.230799</td>\n",
       "      <td>-2.938535</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:33:59.097</td>\n",
       "      <td>S</td>\n",
       "      <td>22.14</td>\n",
       "      <td>13.6</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:40:21.261</td>\n",
       "      <td>S</td>\n",
       "      <td>22.36</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021442286m035936</td>\n",
       "      <td>220.619578</td>\n",
       "      <td>-3.993391</td>\n",
       "      <td>3</td>\n",
       "      <td>0.935</td>\n",
       "      <td>220.618952</td>\n",
       "      <td>-3.993328</td>\n",
       "      <td>2.2614</td>\n",
       "      <td>220.615993</td>\n",
       "      <td>-3.991748</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:33:59.097</td>\n",
       "      <td>N</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.5</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T05:36:54.041</td>\n",
       "      <td>N</td>\n",
       "      <td>22.07</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106021449003p003225</td>\n",
       "      <td>222.251305</td>\n",
       "      <td>0.540528</td>\n",
       "      <td>7</td>\n",
       "      <td>0.984</td>\n",
       "      <td>222.251090</td>\n",
       "      <td>0.540740</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>222.249251</td>\n",
       "      <td>0.544256</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-06-02T02:27:20.976</td>\n",
       "      <td>S</td>\n",
       "      <td>21.04</td>\n",
       "      <td>47.1</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:06:24.397</td>\n",
       "      <td>S</td>\n",
       "      <td>21.21</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202106051435333m000534</td>\n",
       "      <td>218.888992</td>\n",
       "      <td>-0.092877</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857</td>\n",
       "      <td>218.889396</td>\n",
       "      <td>-0.095045</td>\n",
       "      <td>7.9375</td>\n",
       "      <td>218.890974</td>\n",
       "      <td>-0.090694</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:14:11.170</td>\n",
       "      <td>N</td>\n",
       "      <td>22.36</td>\n",
       "      <td>21.7</td>\n",
       "      <td>24</td>\n",
       "      <td>2021-06-05T02:15:30.763</td>\n",
       "      <td>N</td>\n",
       "      <td>21.86</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ObjectID   RA-OBJECT  DEC-OBJECT  NumberAlerts  MaxSCORE  \\\n",
       "0   A202103221407558m001825  211.982786   -0.306951            12     0.972   \n",
       "0   A202103221408139m033502  212.057952   -3.583947            28     0.954   \n",
       "0   A202103221408412p002445  212.171737    0.412527            49     0.998   \n",
       "0   A202103221408578m005300  212.241200   -0.883300             2     0.855   \n",
       "0   A202103221409059m023156  212.274757   -2.532478            21     0.969   \n",
       "..                      ...         ...         ...           ...       ...   \n",
       "0   T202106021437535m004239  219.473273   -0.710944             6     0.973   \n",
       "0   T202106021440559m025615  220.233290   -2.937658             4     0.940   \n",
       "0   T202106021442286m035936  220.619578   -3.993391             3     0.935   \n",
       "0   T202106021449003p003225  222.251305    0.540528             7     0.984   \n",
       "0   T202106051435333m000534  218.888992   -0.092877             2     0.857   \n",
       "\n",
       "    RA-PSEUDO-HOST  DEC-PSEUDO-HOST  SEP-PSEUDO-HOST  RA-NEIGHBOR-STAR  \\\n",
       "0       211.982614        -0.306946           0.6199        211.983372   \n",
       "0       212.057864        -3.583960           0.3199        212.058798   \n",
       "0       212.171673         0.412394           0.5317        212.174697   \n",
       "0       212.241200        -0.883400           0.3000        212.239800   \n",
       "0       212.274533        -2.532531           0.8290        212.275356   \n",
       "..             ...              ...              ...               ...   \n",
       "0       219.473277        -0.711253           1.1100        219.473195   \n",
       "0       220.233839        -2.935933           6.5148        220.230799   \n",
       "0       220.618952        -3.993328           2.2614        220.615993   \n",
       "0       222.251090         0.540740           1.0845        222.249251   \n",
       "0       218.889396        -0.095045           7.9375        218.890974   \n",
       "\n",
       "    DEC-NEIGHBOR-STAR  ...  Discovery-Round           Discovery-Time  \\\n",
       "0           -0.306315  ...                0  2021-03-22T06:40:19.074   \n",
       "0           -3.586276  ...                0  2021-03-22T08:20:58.209   \n",
       "0            0.411566  ...                0  2021-03-22T06:36:50.928   \n",
       "0           -0.884900  ...                0  2021-03-22T08:17:30.880   \n",
       "0           -2.535003  ...                0  2021-03-22T08:14:02.747   \n",
       "..                ...  ...              ...                      ...   \n",
       "0           -0.714488  ...               23  2021-06-02T02:37:33.855   \n",
       "0           -2.938535  ...               23  2021-06-02T05:33:59.097   \n",
       "0           -3.991748  ...               23  2021-06-02T05:33:59.097   \n",
       "0            0.544256  ...               23  2021-06-02T02:27:20.976   \n",
       "0           -0.090694  ...               24  2021-06-05T02:14:11.170   \n",
       "\n",
       "   Discovery-Filter Discovery-Magnitude  Discovery-SNR  Latest-Round  \\\n",
       "0                 N               22.13           19.2             9   \n",
       "0                 N               21.78           25.7            23   \n",
       "0                 S               20.61           36.2            23   \n",
       "0                 S               22.55           15.5             1   \n",
       "0                 N               22.27           10.6            18   \n",
       "..              ...                 ...            ...           ...   \n",
       "0                 N               21.46           30.6            24   \n",
       "0                 S               22.14           13.6            23   \n",
       "0                 N               22.15           22.5            23   \n",
       "0                 S               21.04           47.1            24   \n",
       "0                 N               22.36           21.7            24   \n",
       "\n",
       "                Latest-Time Latest-Filter Latest-Magnitude  Latest-SNR  \n",
       "0   2021-04-18T05:37:55.763             N            22.86        10.2  \n",
       "0   2021-06-02T06:00:51.873             N            21.82        18.1  \n",
       "0   2021-06-02T02:59:21.867             S            21.46        33.9  \n",
       "0   2021-03-24T07:10:51.368             S            22.33        25.6  \n",
       "0   2021-05-18T03:17:17.544             N            22.99         8.8  \n",
       "..                      ...           ...              ...         ...  \n",
       "0   2021-06-05T02:15:30.763             N            21.44        19.8  \n",
       "0   2021-06-02T05:40:21.261             S            22.36        14.0  \n",
       "0   2021-06-02T05:36:54.041             N            22.07        10.5  \n",
       "0   2021-06-05T02:06:24.397             S            21.21        18.4  \n",
       "0   2021-06-05T02:15:30.763             N            21.86        14.9  \n",
       "\n",
       "[557 rows x 21 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decam_transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run initial check (on tiles) and closer check (on targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_matches_by_date = initial_check(ledger_df = decam_transients, ledger_type = 'DECAM_TAMU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No far matches fed in for nearby matching. Returning none.\n"
     ]
    }
   ],
   "source": [
    "#exclusion_list = [20210101, 20210115, 20210205, 20210208, 20210217, 20210218, 20210402, 20210411, 20210428, 20210530]\n",
    "close_matches = closer_check(init_matches_by_date, ledger_df = decam_transients, ledger_type = 'DECAM_TAMU', exclusion_list = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write matches to file per function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "smushed_YMD = today.iso.split(\" \")[0].replace(\"-\",\"\")\n",
    "write_matches_to_file(start_date = \"20201130\", end_date = smushed_YMD, all_matches_dict = close_matches, ledger_type = \"DECAM_TAMU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read matches back in and prepare for CNN\n",
    "Just in case the kernel restarts because all this matching takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matches_file(filename: str) -> dict:\n",
    "    info_dict = {}\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            all_lines = f.readlines()[2:] # Don't need header keywords - they're just there for humans (darned humans)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not open or read:\", filename)\n",
    "#        print(\"Trying the next file...\")\n",
    "        return info_dict\n",
    "        \n",
    "    # Grabbing info from every line\n",
    "    # The structure is human readable but not easy to parse\n",
    "    # Ya win some ya lose some\n",
    "    for line_idx in range(len(all_lines)):\n",
    "        \n",
    "        if \":\" in all_lines[line_idx]: # If even and 0\n",
    "            date = all_lines[line_idx].split(':')[0] # Gets rid of ':' and newline character\n",
    "            info_dict[date] = {}\n",
    "        else:\n",
    "            data = all_lines[line_idx].lstrip('\\t').rstrip('\\n').replace(\" \", \"\").split(';')\n",
    "            info_dict[date][data[0]] = data[1:]\n",
    "    \n",
    "    return info_dict # now a dict of dicts, yippee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "# To be as general as possible we grab all the filenames for all the matches files generated\n",
    "file_list = glob.glob(\"./matches/*.txt\")\n",
    "# But then take the most recent one\n",
    "file_dict = read_matches_file(max(file_list, key=os.path.getctime))\n",
    "#print(file_dict)\n",
    "\n",
    "# It's much easier to just use SQL again here\n",
    "query_template = \"SELECT distinct obsdate, tileid from exposures where expid == \" #obsdate>20210228 \n",
    "\n",
    "'''\n",
    "There's that handy code again for SQL column info\n",
    "#query2 = \"PRAGMA table_info(exposures)\"\n",
    "\n",
    "#cur.execute(query2)\n",
    "#row2 = cur.fetchall()\n",
    "#for i in row2:\n",
    "#    print(i[:])\n",
    "'''\n",
    "\n",
    "conn = sqlite3.connect(db_filename)\n",
    "conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "cur = conn.cursor()\n",
    "\n",
    "info_list = []\n",
    "info_list_w_dup = []\n",
    "exp_id_triplets = []\n",
    "\n",
    "# Iterate through the information read in\n",
    "# Again not too intuitive but oh well\n",
    "for date, v_dict in file_dict.items():\n",
    "    for filename, info in v_dict.items():\n",
    "        \n",
    "        # Grab exposure ID\n",
    "        exp_id = filename.strip(\"\\t\").split('-')[-1][3:-5] # [:-5] to avoid retained '.fits' at end\n",
    "        # Grab row number\n",
    "        row = info[1]\n",
    "        # Grab petal number\n",
    "        petal_num = filename.split(\"-\")[1][1] # First split \"cframe, [color_band][petal_num], [exp_id].fits\"\n",
    "        # Set up the query\n",
    "        query = query_template + exp_id + \";\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        tile_id = cur.fetchone()['tileid']\n",
    "        \n",
    "        # I know grab the targetid in modified_cnn_classify since it's in the zbest file anyway\n",
    "        # Save myself the hassle\n",
    "        \n",
    "        # Don't need the coadd filename here, zbest is zbest\n",
    "        #coadd_filename = \"-\".join((\"coadd\", petal_num, str(row_data['tileid']), date)) + \".fits\"\n",
    "        zbest_filename = \"-\".join((\"zbest\", petal_num, str(tile_id), date)) + \".fits\"\n",
    "        #print(tile_id, date, petal_num, row)\n",
    "        \n",
    "        # Retain duplicates just in case\n",
    "        info_list_w_dup.append((tile_id, date, petal_num, row))\n",
    "        if (tile_id, petal_num, row) in exp_id_triplets:\n",
    "            pass\n",
    "        else:\n",
    "            # Otherwise create the information list here\n",
    "            # There are other ways to do this but hey we're here\n",
    "            info_list.append((tile_id, date, petal_num, row))\n",
    "            exp_id_triplets.append((tile_id, petal_num, row))\n",
    "    \n",
    "cur.close()\n",
    "print(len(info_list))\n",
    "print(len(info_list_w_dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write CNN prepared info to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_feed.txt', 'w') as f:    \n",
    "    for i in info_list:\n",
    "        f.write(str(i).strip(\"()\").replace(\"'\", \"\"))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End notes:\n",
    "Double matches are to be expected, could be worthwhile to compare the spectra of both"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
