{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desispec.io import read_spectra\n",
    "from desitrip.preproc import rebin_flux, rescale_flux\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from astropy.table import Table\n",
    "\n",
    "import os\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc('font', size=14)\n",
    "# np.seterr(all='raise')\n",
    "# np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_spectra(coadd_files, truth_files):\n",
    "    \"\"\"Read DESI spectra, rebin to a subsampled logarithmic wavelength grid, and rescale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coadd_files : list or ndarray\n",
    "        List of FITS files on disk with DESI spectra.\n",
    "    truth_files : list or ndarray\n",
    "        Truth files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fluxes : ndarray\n",
    "        Array of fluxes rebinned to a logarithmic wavelength grid.\n",
    "    \"\"\"\n",
    "    fluxes = None\n",
    "    \n",
    "    for cf, tf in zip(coadd_files, truth_files):\n",
    "        spectra = read_spectra(cf)\n",
    "        wave = spectra.wave['brz']\n",
    "        flux = spectra.flux['brz']\n",
    "        ivar = spectra.ivar['brz']\n",
    "        \n",
    "#         truth = Table.read(tf, 'TRUTH')\n",
    "#         truez = truth['TRUEZ']\n",
    "\n",
    "#         # Pre-condition: remove spectra with NaNs and zero flux values.\n",
    "#         mask = np.isnan(flux).any(axis=1) | (np.count_nonzero(flux, axis=1) == 0)\n",
    "#         mask_idx = np.argwhere(mask)\n",
    "#         flux = np.delete(flux, mask_idx, axis=0)\n",
    "#         ivar = np.delete(ivar, mask_idx, axis=0)\n",
    "\n",
    "#         # Rebin and rescale fluxes so that each is normalized between 0 and 1.\n",
    "#         rewave, reflux, reivar = rebin_flux(wave, flux, ivar, truez, minwave=2500., maxwave=9500., nbins=150, log=True, clip=True)\n",
    "#         rsflux = rescale_flux(reflux)\n",
    "\n",
    "        if fluxes is None:\n",
    "            fluxes = flux\n",
    "        else:\n",
    "            fluxes = np.concatenate((fluxes, flux))\n",
    "    \n",
    "    return_flux=[]\n",
    "    for i in range(len(fluxes)):\n",
    "        trial=fluxes[i][0:6241]\n",
    "        trial=((trial-np.min(trial))/(np.max(trial) - np.min(trial)))\n",
    "        trial=trial.reshape(79,79)\n",
    "        return_flux.append(trial)\n",
    "        \n",
    "    bad_host_counter=0\n",
    "    final_flux=[]\n",
    "    for i in return_flux:\n",
    "        if np.sum(i)==0 or math.isnan(np.sum(i)):\n",
    "            bad_host_counter=bad_host_counter+1\n",
    "        else:\n",
    "            final_flux.append(i)\n",
    "    \n",
    "    return final_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/hosts/*truth.fits'))\n",
    "host_coadd = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/hosts/*coadd.fits'))\n",
    "host_flux  = condition_spectra(host_coadd, host_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snia_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ia/hsiao/*truth.fits'))\n",
    "snia_files = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ia/hsiao/*coadd.fits'))\n",
    "snia_flux  = condition_spectra(snia_files, snia_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snib_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ib/*/*truth.fits'))\n",
    "snib_files = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ib/*/*coadd.fits'))\n",
    "snib_flux  = condition_spectra(snib_files, snib_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "snic_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ic/*/*truth.fits'))\n",
    "snic_files = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_ic/*/*coadd.fits'))\n",
    "snic_flux  = condition_spectra(snic_files, snic_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniin_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_iin/*/*truth.fits'))\n",
    "sniin_files = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_iin/*/*coadd.fits'))\n",
    "sniin_flux  = condition_spectra(sniin_files, sniin_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniip_truth = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_iip/*/*truth.fits'))\n",
    "sniip_files = sorted(glob('/scratch/sbenzvi_lab/desi/time-domain/bgs/150s/sn_iip/*/*coadd.fits'))\n",
    "sniip_flux  = condition_spectra(sniip_files, sniip_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9969, 9964, 9958, 8269, 9949, 9962, 79, 79)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_flux=np.asarray(host_flux)\n",
    "snia_flux=np.asarray(snia_flux)\n",
    "snib_flux=np.asarray(snib_flux)\n",
    "snic_flux=np.asarray(snic_flux)\n",
    "sniin_flux=np.asarray(sniin_flux)\n",
    "sniip_flux=np.asarray(sniip_flux)\n",
    "\n",
    "nhost, nylen, nxlen  = host_flux.shape\n",
    "nsnia, nylen, nxlen  = snia_flux.shape\n",
    "nsnib, nylen, nxlen  = snib_flux.shape\n",
    "nsnic, nylen, nxlen  = snic_flux.shape\n",
    "nsniin, nylen, nxlen = sniin_flux.shape\n",
    "nsniip, nylen, nxlen = sniip_flux.shape\n",
    "# nhost, nsnia, nsnib, nsnibc, nsnic, nsniin, nsniilp, nsniip, nbins\n",
    "nhost, nsnia, nsnib, nsnic, nsniin, nsniip, nylen, nxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/software/python3/3.6.12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/software/desi/b1/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate([host_flux, \n",
    "                    snia_flux,\n",
    "                    snib_flux,\n",
    "                    snic_flux,\n",
    "                    sniin_flux,\n",
    "                    sniip_flux\n",
    "                   ]).reshape(-1, nylen, nxlen, 1)\n",
    "\n",
    "labels = ['Host',\n",
    "          'SN Ia',\n",
    "          'SN Ib',\n",
    "          'SN Ic',\n",
    "          'SN IIn',\n",
    "          'SN IIP']\n",
    "ntypes = len(labels)\n",
    "\n",
    "# Convert y-label array to appropriate categorical array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(\n",
    "        np.concatenate([np.full(nhost, 0), \n",
    "                        np.full(nsnia, 1),\n",
    "                        np.full(nsnib, 2),\n",
    "                        np.full(nsnic, 3),\n",
    "                        np.full(nsniin, 4),\n",
    "                        np.full(nsniip, 5)\n",
    "                       ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_validate(x, y, train_size=0.75, test_size=0.125, val_size=0.125):\n",
    "    # Ensure proper normalization.\n",
    "    if train_size + test_size + val_size != 1:\n",
    "        norm = train_size + test_size + val_size\n",
    "        train_size = train_size/norm\n",
    "        test_size = test_size/norm\n",
    "        val_size = val_size/norm\n",
    "        print('Renormalized to train {:g}, test {:g}, and validate {:g}'.format(train_size, test_size, val_size))\n",
    "        \n",
    "    # Split into training and testing samples.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1-train_size)\n",
    "    \n",
    "    # Split off the validation sample from the test sample.\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_size/(test_size+val_size))\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (34842, 79, 79, 1)\n",
      "34842 train samples\n",
      "11615 test samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((34842, 79, 79, 1), (11615, 79, 79, 1), (11614, 79, 79, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_x, img_y = 79, 79\n",
    "x_train, x_test, x_val, y_train, y_test, y_val = train_test_validate(x, y, 0.6, 0.2, 0.2)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], img_x, img_y, 1)\n",
    "input_shape = (img_x, img_y, 1)\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "x_train.shape, x_test.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 6\n",
    "epochs = 30\n",
    "\n",
    "# input image dimensions\n",
    "img_x, img_y = 79, 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(79,79,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(8, kernel_size=(2, 2), strides=(1, 1),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# model.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1000, activation='relu'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "#               optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34842 samples, validate on 11615 samples\n",
      "Epoch 1/30\n",
      "34842/34842 [==============================] - 1064s 31ms/step - loss: 1.7896 - acc: 0.1724 - val_loss: 1.7909 - val_acc: 0.1714\n",
      "Epoch 2/30\n",
      "34842/34842 [==============================] - 1085s 31ms/step - loss: 1.7894 - acc: 0.1699 - val_loss: 1.7904 - val_acc: 0.1714\n",
      "Epoch 3/30\n",
      "34842/34842 [==============================] - 1111s 32ms/step - loss: 1.7894 - acc: 0.1709 - val_loss: 1.7907 - val_acc: 0.1714\n",
      "Epoch 4/30\n",
      "34842/34842 [==============================] - 1163s 33ms/step - loss: 1.7894 - acc: 0.1703 - val_loss: 1.7905 - val_acc: 0.1714\n",
      "Epoch 5/30\n",
      "34842/34842 [==============================] - 1167s 33ms/step - loss: 1.7893 - acc: 0.1699 - val_loss: 1.7911 - val_acc: 0.1714\n",
      "Epoch 6/30\n",
      "34842/34842 [==============================] - 1161s 33ms/step - loss: 1.7894 - acc: 0.1712 - val_loss: 1.7909 - val_acc: 0.1664\n",
      "Epoch 7/30\n",
      "34842/34842 [==============================] - 1056s 30ms/step - loss: 1.7894 - acc: 0.1705 - val_loss: 1.7911 - val_acc: 0.1664\n",
      "Epoch 8/30\n",
      "34842/34842 [==============================] - 1051s 30ms/step - loss: 1.7893 - acc: 0.1708 - val_loss: 1.7907 - val_acc: 0.1684\n",
      "Epoch 9/30\n",
      "34842/34842 [==============================] - 1050s 30ms/step - loss: 1.7893 - acc: 0.1669 - val_loss: 1.7905 - val_acc: 0.1684\n",
      "Epoch 10/30\n",
      "34842/34842 [==============================] - 1056s 30ms/step - loss: 1.7893 - acc: 0.1661 - val_loss: 1.7909 - val_acc: 0.1684\n",
      "Epoch 11/30\n",
      "34842/34842 [==============================] - 1064s 31ms/step - loss: 1.7893 - acc: 0.1678 - val_loss: 1.7906 - val_acc: 0.1714\n",
      "Epoch 12/30\n",
      "34842/34842 [==============================] - 1062s 30ms/step - loss: 1.7893 - acc: 0.1728 - val_loss: 1.7906 - val_acc: 0.1684\n",
      "Epoch 13/30\n",
      "34842/34842 [==============================] - 1064s 31ms/step - loss: 1.7893 - acc: 0.1707 - val_loss: 1.7907 - val_acc: 0.1684\n",
      "Epoch 14/30\n",
      " 1152/34842 [..............................] - ETA: 15:27 - loss: 1.7903 - acc: 0.1875"
     ]
    }
   ],
   "source": [
    "class AccuracyHistory(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (anaconda3 5.3.0 mine)",
   "language": "python",
   "name": "anaconda3-5.3.0-mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
